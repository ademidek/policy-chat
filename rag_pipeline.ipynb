{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG Application with AWS Bedrock & ChromaDB (Cloud)\n",
                "## Phase 1: Setup & Configuration\n",
                "This notebook covers the setup of dependencies, configuration of credentials, and initialization of AWS Bedrock and ChromaDB Cloud clients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: boto3 in ./venv/lib/python3.13/site-packages (1.42.23)\n",
                        "Requirement already satisfied: chromadb in ./venv/lib/python3.13/site-packages (1.4.0)\n",
                        "Collecting langchain\n",
                        "  Downloading langchain-1.2.1-py3-none-any.whl.metadata (4.9 kB)\n",
                        "Collecting langchain-community\n",
                        "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
                        "Collecting langchain-aws\n",
                        "  Downloading langchain_aws-1.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
                        "Collecting langchain-text-splitters\n",
                        "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
                        "Requirement already satisfied: botocore<1.43.0,>=1.42.23 in ./venv/lib/python3.13/site-packages (from boto3) (1.42.23)\n",
                        "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./venv/lib/python3.13/site-packages (from boto3) (1.0.1)\n",
                        "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in ./venv/lib/python3.13/site-packages (from boto3) (0.16.0)\n",
                        "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.13/site-packages (from chromadb) (1.3.0)\n",
                        "Requirement already satisfied: pydantic>=1.9 in ./venv/lib/python3.13/site-packages (from chromadb) (2.12.5)\n",
                        "Requirement already satisfied: pybase64>=1.4.1 in ./venv/lib/python3.13/site-packages (from chromadb) (1.4.3)\n",
                        "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
                        "Requirement already satisfied: numpy>=1.22.5 in ./venv/lib/python3.13/site-packages (from chromadb) (2.4.0)\n",
                        "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./venv/lib/python3.13/site-packages (from chromadb) (5.4.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.5.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.15.0)\n",
                        "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.13/site-packages (from chromadb) (1.23.2)\n",
                        "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.13/site-packages (from chromadb) (0.22.2)\n",
                        "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.13/site-packages (from chromadb) (0.48.9)\n",
                        "Requirement already satisfied: tqdm>=4.65.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.67.1)\n",
                        "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.13/site-packages (from chromadb) (7.7.0)\n",
                        "Requirement already satisfied: importlib-resources in ./venv/lib/python3.13/site-packages (from chromadb) (6.5.2)\n",
                        "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.13/site-packages (from chromadb) (1.76.0)\n",
                        "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb) (5.0.0)\n",
                        "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.13/site-packages (from chromadb) (0.21.1)\n",
                        "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.13/site-packages (from chromadb) (34.1.0)\n",
                        "Requirement already satisfied: tenacity>=8.2.3 in ./venv/lib/python3.13/site-packages (from chromadb) (9.1.2)\n",
                        "Requirement already satisfied: pyyaml>=6.0.0 in ./venv/lib/python3.13/site-packages (from chromadb) (6.0.3)\n",
                        "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.13/site-packages (from chromadb) (5.2.0)\n",
                        "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.13/site-packages (from chromadb) (3.11.5)\n",
                        "Requirement already satisfied: httpx>=0.27.0 in ./venv/lib/python3.13/site-packages (from chromadb) (0.28.1)\n",
                        "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.13/site-packages (from chromadb) (14.2.0)\n",
                        "Requirement already satisfied: jsonschema>=4.19.0 in ./venv/lib/python3.13/site-packages (from chromadb) (4.26.0)\n",
                        "Collecting langchain-core<2.0.0,>=1.2.1 (from langchain)\n",
                        "  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\n",
                        "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
                        "  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\n",
                        "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
                        "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
                        "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
                        "  Downloading sqlalchemy-2.0.45-py3-none-any.whl.metadata (9.5 kB)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.32.5 in ./venv/lib/python3.13/site-packages (from langchain-community) (2.32.5)\n",
                        "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
                        "  Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
                        "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
                        "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
                        "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
                        "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
                        "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
                        "  Downloading langsmith-0.6.1-py3-none-any.whl.metadata (15 kB)\n",
                        "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
                        "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
                        "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
                        "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
                        "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (20 kB)\n",
                        "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Using cached multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
                        "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
                        "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
                        "  Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (75 kB)\n",
                        "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./venv/lib/python3.13/site-packages (from botocore<1.43.0,>=1.42.23->boto3) (2.9.0.post0)\n",
                        "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./venv/lib/python3.13/site-packages (from botocore<1.43.0,>=1.42.23->boto3) (2.3.0)\n",
                        "Requirement already satisfied: packaging>=19.1 in ./venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (25.0)\n",
                        "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.13/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
                        "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
                        "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
                        "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
                        "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
                        "Requirement already satisfied: anyio in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (4.12.1)\n",
                        "Requirement already satisfied: certifi in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (2026.1.4)\n",
                        "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
                        "Requirement already satisfied: idna in ./venv/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
                        "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
                        "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
                        "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
                        "Requirement already satisfied: rpds-py>=0.25.0 in ./venv/lib/python3.13/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
                        "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
                        "Requirement already satisfied: google-auth>=1.0.1 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.47.0)\n",
                        "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
                        "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
                        "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
                        "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
                        "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
                        "Collecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.2.1->langchain)\n",
                        "  Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (1.1 kB)\n",
                        "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
                        "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
                        "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
                        "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
                        "Collecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
                        "  Downloading langgraph_sdk-0.3.1-py3-none-any.whl.metadata (1.6 kB)\n",
                        "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
                        "  Downloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (13 kB)\n",
                        "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
                        "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
                        "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
                        "  Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
                        "Requirement already satisfied: coloredlogs in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
                        "Requirement already satisfied: flatbuffers in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
                        "Requirement already satisfied: protobuf in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.2)\n",
                        "Requirement already satisfied: sympy in ./venv/lib/python3.13/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
                        "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.13/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
                        "Requirement already satisfied: googleapis-common-protos~=1.57 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-proto==1.39.1 in ./venv/lib/python3.13/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in ./venv/lib/python3.13/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
                        "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
                        "Requirement already satisfied: distro>=1.5.0 in ./venv/lib/python3.13/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (2.41.5)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.13/site-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
                        "Requirement already satisfied: python-dotenv>=0.21.0 in ./venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.13/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.13/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
                        "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.13/site-packages (from tokenizers>=0.13.2->chromadb) (1.2.4)\n",
                        "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.13/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
                        "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
                        "Requirement already satisfied: uvloop>=0.15.1 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
                        "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
                        "Requirement already satisfied: websockets>=10.4 in ./venv/lib/python3.13/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.13/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
                        "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.2)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.12.0)\n",
                        "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
                        "Requirement already satisfied: typer-slim in ./venv/lib/python3.13/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (0.21.1)\n",
                        "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.13/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
                        "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.13/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
                        "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
                        "  Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (3.2 kB)\n",
                        "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
                        "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
                        "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
                        "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.13/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
                        "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.13/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
                        "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./venv/lib/python3.13/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
                        "Downloading langchain-1.2.1-py3-none-any.whl (105 kB)\n",
                        "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading langchain_aws-1.2.0-py3-none-any.whl (153 kB)\n",
                        "Downloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
                        "Downloading aiohttp-3.13.3-cp313-cp313-macosx_11_0_arm64.whl (490 kB)\n",
                        "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
                        "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
                        "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
                        "Downloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n",
                        "Downloading langsmith-0.6.1-py3-none-any.whl (282 kB)\n",
                        "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
                        "Downloading sqlalchemy-2.0.45-py3-none-any.whl (1.9 MB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
                        "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
                        "Downloading frozenlist-1.8.0-cp313-cp313-macosx_11_0_arm64.whl (49 kB)\n",
                        "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
                        "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
                        "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
                        "Downloading langgraph_sdk-0.3.1-py3-none-any.whl (66 kB)\n",
                        "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
                        "Using cached multidict-6.7.0-cp313-cp313-macosx_11_0_arm64.whl (43 kB)\n",
                        "Using cached propcache-0.4.1-cp313-cp313-macosx_11_0_arm64.whl (46 kB)\n",
                        "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
                        "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
                        "Downloading uuid_utils-0.12.0-cp39-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (603 kB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m603.2/603.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[?25hDownloading xxhash-3.6.0-cp313-cp313-macosx_11_0_arm64.whl (30 kB)\n",
                        "Using cached yarl-1.22.0-cp313-cp313-macosx_11_0_arm64.whl (93 kB)\n",
                        "Downloading zstandard-0.25.0-cp313-cp313-macosx_11_0_arm64.whl (640 kB)\n",
                        "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m640.4/640.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
                        "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
                        "Downloading ormsgpack-1.12.1-cp313-cp313-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
                        "Installing collected packages: zstandard, xxhash, uuid-utils, SQLAlchemy, propcache, ormsgpack, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, frozenlist, aiohappyeyeballs, yarl, typing-inspect, requests-toolbelt, aiosignal, pydantic-settings, langsmith, langgraph-sdk, dataclasses-json, aiohttp, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-aws, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain\n",
                        "Successfully installed SQLAlchemy-2.0.45 aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.8.0 httpx-sse-0.4.3 jsonpatch-1.33 langchain-1.2.1 langchain-aws-1.2.0 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-core-1.2.6 langchain-text-splitters-1.1.0 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.1 langsmith-0.6.1 marshmallow-3.26.2 multidict-6.7.0 mypy-extensions-1.1.0 ormsgpack-1.12.1 propcache-0.4.1 pydantic-settings-2.12.0 requests-toolbelt-1.0.0 typing-inspect-0.9.0 uuid-utils-0.12.0 xxhash-3.6.0 yarl-1.22.0 zstandard-0.25.0\n",
                        "\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
                        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: Install Dependencies\n",
                "# Using %pip ensures packages are installed in the current Jupyter kernel\n",
                "%pip install boto3 chromadb langchain langchain-community langchain-aws langchain-text-splitters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration Loaded.\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Configuration & Variables\n",
                "import os\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "\n",
                "# --- AWS Configuration ---\n",
                "# PLEASE REPLACE WITH YOUR ACTUAL CREDENTIALS\n",
                "AWS_ACCESS_KEY_ID = \"AKIAVFIWI7VEZTWFO7XK\"\n",
                "AWS_SECRET_ACCESS_KEY = \"VQyTFeWxhmjPhZ5lDlLTRuir4YaWrRfp+Xr7omwC\"\n",
                "AWS_REGION = \"us-east-2\"\n",
                "\n",
                "# --- Bedrock Model Configuration ---\n",
                "# Using a stable Claude 3 Sonnet ID which is widely available in us-west-2\n",
                "BEDROCK_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
                "\n",
                "# --- ChromaDB Cloud Configuration ---\n",
                "# Sign up at https://trychroma.com to get your API Token\n",
                "CHROMA_API_KEY = \"ck-spmpTTkferWXYndpC8oa19kuaqRC4cRJNWgFsspdEAh\"\n",
                "CHROMA_TENANT = \"default_tenant\"  # Usually 'default_tenant' for most users\n",
                "CHROMA_DATABASE = \"rag_demo1\" # Usually 'default_database'\n",
                "CHROMA_COLLECTION_NAME = \"rag_collection\"\n",
                "\n",
                "# Apply Environment Variables for Boto3\n",
                "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
                "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
                "os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION\n",
                "\n",
                "print(\"Configuration Loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1. Initializing Boto3 Session...\n",
                        "   âœ… Bedrock Client Initialized successfully.\n",
                        "\n",
                        "2. Initializing ChromaDB Cloud Client...\n",
                        "   âœ… Connected to Chroma Cloud. Collection 'rag_collection' ready.\n",
                        "   â„¹ï¸ Current Collection Count: 0\n"
                    ]
                }
            ],
            "source": [
                "# Step 3: Initialize Clients\n",
                "import boto3\n",
                "import chromadb\n",
                "\n",
                "print(\"1. Initializing Boto3 Session...\")\n",
                "try:\n",
                "    session = boto3.Session(\n",
                "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
                "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
                "        region_name=AWS_REGION\n",
                "    )\n",
                "    bedrock_client = session.client(\"bedrock-runtime\")\n",
                "    print(\"   âœ… Bedrock Client Initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"   âŒ Error initializing Bedrock: {e}\")\n",
                "\n",
                "print(\"\\n2. Initializing ChromaDB Cloud Client...\")\n",
                "try:\n",
                "    # Initialize CloudClient specifically for Chroma Cloud\n",
                "    chroma_client = chromadb.CloudClient(\n",
                "        tenant=CHROMA_TENANT,\n",
                "        database=CHROMA_DATABASE,\n",
                "        api_key=CHROMA_API_KEY\n",
                "    )\n",
                "    \n",
                "    # Get or create the collection\n",
                "    collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
                "    print(f\"   âœ… Connected to Chroma Cloud. Collection '{CHROMA_COLLECTION_NAME}' ready.\")\n",
                "    print(f\"   â„¹ï¸ Current Collection Count: {collection.count()}\")\n",
                "except Exception as e:\n",
                "    print(f\"   âŒ Error initializing ChromaDB Cloud: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Data Ingestion & Chunking\n",
                "We will read text files from the `files/` directory, chunk them using LangChain's `RecursiveCharacterTextSplitter`, save the chunks to `files/chunked/`, and verify the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "â„¹ï¸ Directory exists: Richmond_Policies_Cleaned/chunked\n",
                        "Found 95 files in Richmond_Policies_Cleaned: ['jury_duty_and_subpoenas_policy.txt', 'endowment_spending_policy.txt', 'policy_on_pregnancy_childbirth_lactation_and_related_conditions_faculty_and_staff1.txt', 'password_policy.txt', 'policy_on_provision_of_financial_resources_to_students.txt'] ...\n"
                    ]
                }
            ],
            "source": [
                "# Step 4: Setup Directories\n",
                "import os\n",
                "\n",
                "SOURCE_DIR = \"Richmond_Policies_Cleaned\"\n",
                "CHUNKED_DIR = os.path.join(SOURCE_DIR, \"chunked\")\n",
                "\n",
                "# Create chunked directory if it doesn't exist\n",
                "if not os.path.exists(CHUNKED_DIR):\n",
                "    os.makedirs(CHUNKED_DIR)\n",
                "    print(f\"âœ… Created directory: {CHUNKED_DIR}\")\n",
                "else:\n",
                "    print(f\"â„¹ï¸ Directory exists: {CHUNKED_DIR}\")\n",
                "\n",
                "# List source files (excluding directory or hidden files)\n",
                "source_files = [f for f in os.listdir(SOURCE_DIR) if os.path.isfile(os.path.join(SOURCE_DIR, f)) and not f.startswith('.')]\n",
                "print(f\"Found {len(source_files)} files in {SOURCE_DIR}: {source_files[:5]} ...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting chunking process...\n",
                        "\n",
                        "âœ… jury_duty_and_subpoenas_policy.txt: Created 3 chunks.\n",
                        "âœ… endowment_spending_policy.txt: Created 4 chunks.\n",
                        "âœ… policy_on_pregnancy_childbirth_lactation_and_related_conditions_faculty_and_staff1.txt: Created 9 chunks.\n",
                        "âœ… password_policy.txt: Created 11 chunks.\n",
                        "âœ… policy_on_provision_of_financial_resources_to_students.txt: Created 13 chunks.\n",
                        "âœ… course_level_policy.txt: Created 4 chunks.\n",
                        "âœ… bereavement_leave_policy.txt: Created 5 chunks.\n",
                        "âœ… policy_for_events_with_alcohol_on_campus.txt: Created 22 chunks.\n",
                        "âœ… alcohol_and_drug_policy.txt: Created 53 chunks.\n",
                        "âœ… policy_on_space_allocation_and_facilities_resources.txt: Created 34 chunks.\n",
                        "âœ… multiple_donor_gifts_policy.txt: Created 7 chunks.\n",
                        "âœ… general_data_privacy_regulation_notice.txt: Created 34 chunks.\n",
                        "âœ… policy_for_employment_of_out_of_state_residents.txt: Created 13 chunks.\n",
                        "âœ… non-retaliation_policy.txt: Created 4 chunks.\n",
                        "âœ… office_assignment_policy.txt: Created 13 chunks.\n",
                        "âœ… official_university_communications_policy.txt: Created 2 chunks.\n",
                        "âœ… placement_of_student_art_installations.txt: Created 7 chunks.\n",
                        "âœ… external_data_transfer_policy.txt: Created 6 chunks.\n",
                        "âœ… policy_on_prohibiting_and_responding_to_sexual_harassment_and_sexual_misconduct_students.txt: Created 117 chunks.\n",
                        "âœ… administrative_data_management_policy.txt: Created 29 chunks.\n",
                        "âœ… inclement_weather_time_reporting_and_pay_policy.txt: Created 10 chunks.\n",
                        "âœ… policy_on_political_campaign_activity_on_campus.txt: Created 26 chunks.\n",
                        "âœ… intellectual_property_policy.txt: Created 49 chunks.\n",
                        "âœ… policy_for_responding_to_allegations_of_research_misconduct.txt: Created 92 chunks.\n",
                        "âœ… academic_integrity_monitoring.txt: Created 8 chunks.\n",
                        "âœ… policy_on_prohibiting_and_responding_to_sex_discrimination_faculty_staff.txt: Created 71 chunks.\n",
                        "âœ… museum_collections_management_policy.txt: Created 44 chunks.\n",
                        "âœ… contract_management_policy.txt: Created 22 chunks.\n",
                        "âœ… space_taxonomy.txt: Created 3 chunks.\n",
                        "âœ… network_device_connectivity_policy.txt: Created 10 chunks.\n",
                        "âœ… policy_prohibiting_discrimination.txt: Created 22 chunks.\n",
                        "âœ… film_and_media_screening_policy.txt: Created 5 chunks.\n",
                        "âœ… Statement_on_Free_Expression.txt: Created 5 chunks.\n",
                        "âœ… policy_on_business_expenses_and_compensation.txt: Created 7 chunks.\n",
                        "âœ… catering_minimums_policy.txt: Created 7 chunks.\n",
                        "âœ… financial_aid_code_of_conduct.txt: Created 18 chunks.\n",
                        "âœ… board_of_trustees_conflict_of_interest_policy.txt: Created 39 chunks.\n",
                        "âœ… academic_progress_policy_as_bus_jepson.txt: Created 13 chunks.\n",
                        "âœ… data_security_policy.txt: Created 38 chunks.\n",
                        "âœ… academic_and_professional_preparation_requirements_for_faculty.txt: Created 10 chunks.\n",
                        "âœ… information_security_policy.txt: Created 13 chunks.\n",
                        "âœ… gifts_and_gratuities_policy.txt: Created 5 chunks.\n",
                        "âœ… joint_ventures_policy.txt: Created 5 chunks.\n",
                        "âœ… designation_of_emergency_personnel_policy.txt: Created 12 chunks.\n",
                        "âœ… electronic_signature_policy.txt: Created 12 chunks.\n",
                        "âœ… policy_on_creating_suspending_eliminating_programs.txt: Created 34 chunks.\n",
                        "âœ… hazing_policy.txt: Created 17 chunks.\n",
                        "âœ… hipaa_policy.txt: Created 37 chunks.\n",
                        "âœ… policy_prohibiting_and_responding_to_discrimination_based_on_protected_status_faculty_staff.txt: Created 64 chunks.\n",
                        "âœ… indirect_costs_recovery_policy.txt: Created 6 chunks.\n",
                        "âœ… policy_prohibiting_firearms_on_campus.txt: Created 6 chunks.\n",
                        "âœ… acceptable_use_policy.txt: Created 20 chunks.\n",
                        "âœ… policy_prohibiting_and_responding_to_discrimination_based_on_protected_status_students.txt: Created 64 chunks.\n",
                        "âœ… academic_credit_policy.txt: Created 41 chunks.\n",
                        "âœ… faculty_phased_retirement_plan.txt: Created 10 chunks.\n",
                        "âœ… policy_and_procedure_on_monetary_support_and_cash_donations.txt: Created 11 chunks.\n",
                        "âœ… campus_ministry_policy.txt: Created 12 chunks.\n",
                        "âœ… classroom_scheduling_policy.txt: Created 4 chunks.\n",
                        "âœ… lock_and_key_management_policy.txt: Created 19 chunks.\n",
                        "âœ… policy_on_prohibiting_and_responding_to_sexual_harassment_and_sexual_misconduct_faculty_staff.txt: Created 145 chunks.\n",
                        "âœ… health_and_imunization_record_policy.txt: Created 5 chunks.\n",
                        "âœ… early_retirement_plan_for_staff_and_faculty_with_a_continuing_appointment.txt: Created 24 chunks.\n",
                        "âœ… compliance_training_policy.txt: Created 3 chunks.\n",
                        "âœ… delegation_of_approval_authority_and_processing_responsibilities.txt: Created 13 chunks.\n",
                        "âœ… cell_phone-policy.txt: Created 8 chunks.\n",
                        "âœ… access_to_electronic_files_policy.txt: Created 13 chunks.\n",
                        "âœ… gifts_prizes_and_awards_policy.txt: Created 11 chunks.\n",
                        "âœ… delegation_of_contract_approval_and_signature_authority_policy.txt: Created 38 chunks.\n",
                        "âœ… military_leave_policy.txt: Created 3 chunks.\n",
                        "âœ… policy_on_policies.txt: Created 8 chunks.\n",
                        "âœ… inclement_weather_policy.txt: Created 8 chunks.\n",
                        "âœ… flexible_work_arrangement_policy.txt: Created 15 chunks.\n",
                        "âœ… financial_conflict_of_interest_for_grant-funded_research.txt: Created 21 chunks.\n",
                        "âœ… use_of_university_owned_houses.txt: Created 11 chunks.\n",
                        "âœ… policy_on_prohibiting_and_responding_to_sex_discrimination_students.txt: Created 69 chunks.\n",
                        "âœ… policy_on_campus_protests_and_demonstrations.txt: Created 18 chunks.\n",
                        "âœ… appointment_of_endowed_chairs.txt: Created 14 chunks.\n",
                        "âœ… effective_use_of_institutional_funds.txt: Created 9 chunks.\n",
                        "âœ… electronic_card_access_policy.txt: Created 12 chunks.\n",
                        "âœ… business_meals_policy.txt: Created 24 chunks.\n",
                        "âœ… international_travel_policy.txt: Created 13 chunks.\n",
                        "âœ… holiday_leave_policy.txt: Created 10 chunks.\n",
                        "âœ… board_of_trustees_contract_approval_and_signature_authority.txt: Created 19 chunks.\n",
                        "âœ… identity_and_access_management_policy.txt: Created 20 chunks.\n",
                        "âœ… nepotism_and_personal_relationship_policy.txt: Created 4 chunks.\n",
                        "âœ… policy_on_emeritus_status.txt: Created 3 chunks.\n",
                        "âœ… cybersecurity_incident_response_policy.txt: Created 16 chunks.\n",
                        "âœ… parental_leave_policy.txt: Created 11 chunks.\n",
                        "âœ… additional_compensation_for_staff.txt: Created 15 chunks.\n",
                        "âœ… photography_and_videography_policy.txt: Created 7 chunks.\n",
                        "âœ… fringe_rate_policy.txt: Created 6 chunks.\n",
                        "âœ… emergency_management_policy.txt: Created 18 chunks.\n",
                        "âœ… policy_on_protecting_student_privacy_in_distance_education.txt: Created 6 chunks.\n",
                        "âœ… benefits_for_10-month_staff_and_faculty_on_9-month_contracts.txt: Created 5 chunks.\n",
                        "âœ… outdoor_grill_use_policy.txt: Created 8 chunks.\n",
                        "\n",
                        "ðŸŽ‰ Total Chunks Created: 1914\n"
                    ]
                }
            ],
            "source": [
                "# Step 5: Load, Chunk, and Save Files\n",
                "try:\n",
                "    # Try modern import first\n",
                "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "except ImportError:\n",
                "    # Fallback to legacy import\n",
                "    try:\n",
                "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "    except ImportError:\n",
                "        # Last resort\n",
                "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "# Initialize Splitter (prioritize sentence boundaries)\n",
                "text_splitter = RecursiveCharacterTextSplitter(\n",
                "    separators=[\". \", \"? \", \"! \", \"\\n\", \" \", \"\"],\n",
                "    chunk_size=1000,      # Characters (~200 tokens)\n",
                "    chunk_overlap=100,    # Overlap to maintain context\n",
                "    length_function=len,\n",
                "    is_separator_regex=False\n",
                ")\n",
                "\n",
                "total_chunks_processed = 0\n",
                "\n",
                "print(\"Starting chunking process...\\n\")\n",
                "\n",
                "for file_name in source_files:\n",
                "    file_path = os.path.join(SOURCE_DIR, file_name)\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "            \n",
                "        # Create Chunks\n",
                "        chunks = text_splitter.split_text(text)\n",
                "        \n",
                "        # Save each chunk with metadata in filename\n",
                "        # Format: ch{index}-{original_name}-{metadata}.txt\n",
                "        base_name = os.path.splitext(file_name)[0]\n",
                "        \n",
                "        for i, chunk_content in enumerate(chunks):\n",
                "            # Metadata example: length of chunk\n",
                "            metadata_str = f\"len{len(chunk_content)}\"\n",
                "            chunk_filename = f\"ch{i+1}-{base_name}-{metadata_str}.txt\"\n",
                "            chunk_path = os.path.join(CHUNKED_DIR, chunk_filename)\n",
                "            \n",
                "            with open(chunk_path, 'w', encoding='utf-8') as chunk_file:\n",
                "                chunk_file.write(chunk_content)\n",
                "                \n",
                "        print(f\"âœ… {file_name}: Created {len(chunks)} chunks.\")\n",
                "        total_chunks_processed += len(chunks)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"âŒ Error processing {file_name}: {e}\")\n",
                "\n",
                "print(f\"\\nðŸŽ‰ Total Chunks Created: {total_chunks_processed}\")\n",
                "\n",
                "## Try and write a prompt to get a list of articles that "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Content of ch50-alcohol_and_drug_policy-len748.txt ---\n",
                        ". resources include one-to-one coaching, recovery literature, mutual aid\n",
                        "meetings, education-based seminars, substance-free activities, and events with other students in\n",
                        "recovery.\n",
                        "the collegiate recovery community is a voluntary, positive, supportive group that respects one\n",
                        "another's privacy. for more information, please visit the webpage at:\n",
                        "https://healthpromotion.richmond.edu/spiders-support-recovery/index.html\n",
                        ".\n",
                        "1002.9 â€“ health risks\n",
                        "the negative physical and mental effects of the use of alc\n",
                        "\n",
                        "--- End of Sample ---\n"
                    ]
                }
            ],
            "source": [
                "# Step 6: Verify a Sample Chunk\n",
                "# Check one of the generated files to ensure content is correct\n",
                "if os.listdir(CHUNKED_DIR):\n",
                "    sample_chunk = os.listdir(CHUNKED_DIR)[0]\n",
                "    sample_path = os.path.join(CHUNKED_DIR, sample_chunk)\n",
                "    \n",
                "    print(f\"--- Content of {sample_chunk} ---\")\n",
                "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
                "        print(f.read()[:500]) # Print first 500 chars\n",
                "    print(\"\\n--- End of Sample ---\")\n",
                "else:\n",
                "    print(\"No chunks found to verify.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Embeddings & Vector Store\n",
                "We will now read the chunked files we just created, generate embeddings (handled automatically by Chroma's default embedding function), and upsert them into the ChromaDB Cloud collection.\n",
                "\n",
                "> **Note:** We are using ChromaDB's default embedding model (`all-MiniLM-L6-v2`) which is built into the client. No extra API calls to Bedrock are needed for *embedding* in this setup, saving costs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 1914 chunk files to process.\n",
                        "Prepared 1914 documents for embedding.\n"
                    ]
                }
            ],
            "source": [
                "# Step 7: Prepare Data for Embedding\n",
                "import uuid\n",
                "import re\n",
                "\n",
                "chunked_files = [f for f in os.listdir(CHUNKED_DIR) if f.endswith('.txt')]\n",
                "\n",
                "documents = []\n",
                "metadatas = []\n",
                "ids = []\n",
                "\n",
                "print(f\"Found {len(chunked_files)} chunk files to process.\")\n",
                "\n",
                "for file_name in chunked_files:\n",
                "    file_path = os.path.join(CHUNKED_DIR, file_name)\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            content = f.read()\n",
                "            \n",
                "        # Parse Metadata from Filename\n",
                "        # Format: ch{index}-{original_name}-{len}.txt\n",
                "        # Example: ch1-academic_policy-len495.txt\n",
                "        try:\n",
                "            name_no_ext = os.path.splitext(file_name)[0]\n",
                "            parts = name_no_ext.split('-')\n",
                "            \n",
                "            # 1. Chunk Part (first item, e.g., 'ch1')\n",
                "            chunk_part = int(parts[0].replace('ch', ''))\n",
                "            \n",
                "            # 2. Size (last item, e.g., 'len495')\n",
                "            size = int(parts[-1].replace('len', ''))\n",
                "            \n",
                "            # 3. File Name (everything in between)\n",
                "            original_filename = \"-\".join(parts[1:-1])\n",
                "            \n",
                "            meta = {\n",
                "                \"source\": file_name,\n",
                "                \"file_name\": original_filename,\n",
                "                \"chunk_part\": chunk_part,\n",
                "                \"size\": size\n",
                "            }\n",
                "        except Exception as e:\n",
                "            # Fallback if naming convention doesn't match\n",
                "            print(f\"âš ï¸ Metadata parse warning for {file_name}: {e}\")\n",
                "            meta = {\"source\": file_name}\n",
                "\n",
                "        # Add to lists\n",
                "        documents.append(content)\n",
                "        metadatas.append(meta)\n",
                "        ids.append(str(uuid.uuid4()))\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not read {file_name}: {e}\")\n",
                "\n",
                "print(f\"Prepared {len(documents)} documents for embedding.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Upserting documents to ChromaDB Collection in batches...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/ademidek/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [00:50<00:00, 1.65MiB/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   âœ… Processed batch 0 to 100\n",
                        "   âœ… Processed batch 100 to 200\n",
                        "   âœ… Processed batch 200 to 300\n",
                        "   âœ… Processed batch 300 to 400\n",
                        "   âœ… Processed batch 400 to 500\n",
                        "   âœ… Processed batch 500 to 600\n",
                        "   âœ… Processed batch 600 to 700\n",
                        "   âœ… Processed batch 700 to 800\n",
                        "   âœ… Processed batch 800 to 900\n",
                        "   âœ… Processed batch 900 to 1000\n",
                        "   âœ… Processed batch 1000 to 1100\n",
                        "   âœ… Processed batch 1100 to 1200\n",
                        "   âœ… Processed batch 1200 to 1300\n",
                        "   âœ… Processed batch 1300 to 1400\n",
                        "   âœ… Processed batch 1400 to 1500\n",
                        "   âœ… Processed batch 1500 to 1600\n",
                        "   âœ… Processed batch 1600 to 1700\n",
                        "   âœ… Processed batch 1700 to 1800\n",
                        "   âœ… Processed batch 1800 to 1900\n",
                        "   âœ… Processed batch 1900 to 1914\n",
                        "\n",
                        "ðŸŽ‰ Successfully added all 1914 documents to ChromaDB!\n",
                        "Final Collection Count: 1914\n"
                    ]
                }
            ],
            "source": [
                "# Step 8: Add to ChromaDB (Embed & Upsert)\n",
                "# Batch size limit for Chroma is usually 1000 (we hit 1914!), so we must batch.\n",
                "print(\"Upserting documents to ChromaDB Collection in batches...\")\n",
                "\n",
                "BATCH_SIZE = 100  # Safe batch size\n",
                "total_docs = len(documents)\n",
                "\n",
                "try:\n",
                "    for i in range(0, total_docs, BATCH_SIZE):\n",
                "        batch_docs = documents[i : i + BATCH_SIZE]\n",
                "        batch_metas = metadatas[i : i + BATCH_SIZE]\n",
                "        batch_ids = ids[i : i + BATCH_SIZE]\n",
                "        \n",
                "        collection.add(\n",
                "            documents=batch_docs,\n",
                "            metadatas=batch_metas,\n",
                "            ids=batch_ids\n",
                "        )\n",
                "        print(f\"   âœ… Processed batch {i} to {min(i+BATCH_SIZE, total_docs)}\")\n",
                "        \n",
                "    print(f\"\\nðŸŽ‰ Successfully added all {total_docs} documents to ChromaDB!\")\n",
                "    print(f\"Final Collection Count: {collection.count()}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"âŒ Error adding to ChromaDB: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 9: Verify Embedding with a Test Query\n",
                "# We will perform a simple similarity search (no LLM yet) to see if we get relevant chunks.\n",
                "\n",
                "query_text = \"What is the main topic of these documents?\"  # Replace with a relevant question for your data\n",
                "\n",
                "print(f\"Querying ChromaDB for: '{query_text}'...\\n\")\n",
                "\n",
                "results = collection.query(\n",
                "    query_texts=[query_text],\n",
                "    n_results=3 # Get top 3 matches\n",
                ")\n",
                "\n",
                "if results['documents']:\n",
                "    for i, doc in enumerate(results['documents'][0]):\n",
                "        meta = results['metadatas'][0][i]\n",
                "        print(f\"[Result {i+1}]\")\n",
                "        print(f\"   File: {meta.get('file_name', 'Unknown')}\")\n",
                "        print(f\"   Part: {meta.get('chunk_part', '?')}\")\n",
                "        print(f\"   Size: {meta.get('size', '?')}\")\n",
                "        print(f\"   Snippet: {doc[:100]}...\\n\")\n",
                "else:\n",
                "    print(\"No results found. Check if documents were added correctly.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Retrieval & Generation\n",
                "We implement the custom retrieval logical (with distince threshold filtering) and connect it to AWS Bedrock for the final answer generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 10: Custom Retrieval Function\n",
                "def retrieve_documents(query, n_results=5, threshold=1.5, filter_by=None):\n",
                "    \"\"\"\n",
                "    Retrieve relevant documents with distance threshold filtering.\n",
                "    \n",
                "    Args:\n",
                "        query: The search query string\n",
                "        n_results: Max results to return initially\n",
                "        threshold: Max distance (lower = more strict match). \n",
                "                   For Cosine distance: 0 is identical, 1 is orthogonal, 2 is opposite.\n",
                "                   Typical good matches are < 1.0 depending on embedding model.\n",
                "        filter_by: Metadata filter dict (optional)\n",
                "    \n",
                "    Returns:\n",
                "        List of dicts: {text, source, distance}\n",
                "    \"\"\"\n",
                "    # Query Chroma\n",
                "    results = collection.query(\n",
                "        query_texts=[query],\n",
                "        n_results=n_results,\n",
                "        where=filter_by,\n",
                "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
                "    )\n",
                "    \n",
                "    docs = []\n",
                "    \n",
                "    # Check if we got results\n",
                "    if results['documents'] and results['documents'][0]:\n",
                "        # Iterate through the first query's results\n",
                "        for text, meta, dist in zip(\n",
                "            results['documents'][0],\n",
                "            results['metadatas'][0],\n",
                "            results['distances'][0]\n",
                "        ):\n",
                "            # Filter by threshold\n",
                "            if dist <= threshold:\n",
                "                docs.append({\n",
                "                    \"text\": text,\n",
                "                    \"source\": meta.get(\"source\", \"unknown\"),\n",
                "                    \"distance\": dist\n",
                "                })\n",
                "                \n",
                "    print(f\"âœ… Retrieved {len(docs)} documents (Threshold: {threshold})\")\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 11: RAG Generation Function (Bedrock)\n",
                "from langchain_aws import ChatBedrock\n",
                "\n",
                "# Fix for newer LangChain versions (v0.1+)\n",
                "try:\n",
                "    from langchain_core.prompts import PromptTemplate\n",
                "    from langchain_core.runnables import RunnablePassthrough\n",
                "    from langchain_core.output_parsers import StrOutputParser\n",
                "except ImportError:\n",
                "    # Fallback for older versions\n",
                "    from langchain.prompts import PromptTemplate\n",
                "    from langchain.schema.runnable import RunnablePassthrough\n",
                "    from langchain.schema.output_parser import StrOutputParser\n",
                "\n",
                "# Initialize LLM\n",
                "llm = ChatBedrock(\n",
                "    model_id=BEDROCK_MODEL_ID,\n",
                "    client=bedrock_client,\n",
                "    model_kwargs={\"max_tokens\": 1000, \"temperature\": 0.1} # Claude 3 uses 'max_tokens'\n",
                ")\n",
                "\n",
                "def generate_answer(query):\n",
                "    # 1. Retrieve Context\n",
                "    relevant_docs = retrieve_documents(query, n_results=5, threshold=1.2)\n",
                "    \n",
                "    if not relevant_docs:\n",
                "        return \"I could not find any relevant information to answer your question.\"\n",
                "    \n",
                "    # 2. Format Context\n",
                "    context_text = \"\\n\\n\".join([f\"[Source: {d['source']}]\\n{d['text']}\" for d in relevant_docs])\n",
                "    \n",
                "    # 3. Construct Prompt\n",
                "    prompt_template = \"\"\"\n",
                "    Human: You are a concise and direct assistant. Use the following pieces of context to answer the question at the end.\n",
                "    \n",
                "    Rules for answering:\n",
                "    1. Be extremely concise.\n",
                "    2. Do NOT use bullet points or numbered lists. \n",
                "    3. Provide a single, direct paragraph.\n",
                "    4. If you don't know the answer, just say that you don't know.\n",
                "\n",
                "    Context:\n",
                "    {context}\n",
                "\n",
                "    Question: {question}\n",
                "\n",
                "    Assistant:\"\"\"\n",
                "    \n",
                "    prompt = PromptTemplate(\n",
                "        template=prompt_template, \n",
                "        input_variables=[\"context\", \"question\"]\n",
                "    )\n",
                "    \n",
                "    # 4. Invoke Chain manually (since we have custom retrieval logic)\n",
                "    # We format the prompt first, then send to LLM\n",
                "    final_prompt = prompt.format(context=context_text, question=query)\n",
                "    response = llm.invoke(final_prompt)\n",
                "    \n",
                "    return response.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 12: Final Test\n",
                "query = \"What is the policy regarding drug usage?\"\n",
                "\n",
                "print(f\"â“ Question: {query}\\n\")\n",
                "\n",
                "answer = generate_answer(query)\n",
                "\n",
                "print(\"ðŸ’¡ Answer:\")\n",
                "print(answer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv (3.13.1)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
